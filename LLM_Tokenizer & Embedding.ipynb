{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "949e99a7-be3e-4752-91eb-b26bb7906112",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# LLM_Tokenizer and Embedding\n",
    "*From Language to Meaningful Vector Representation*\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Tokenizer: 언어를 모델이 이해할 수 있는 단위로 자르는 행위\n",
    "\n",
    "자연어는 연속적인 의미의 흐름으로 이루어져 있다. 그러나 기계는 이 흐름을 그대로 다룰 수 없다. \n",
    "따라서 **언어를 어떤 단위로 끊고 표현할지를 결정하는 전처리 기제**가 필요하다. 이 단위를 \"토큰(token)\"이라 하며, 이 작업을 수행하는 도구를 **Tokenizer**라고 부른다.\n",
    "\n",
    "### 🔹 토크나이저는 단순한 분할 기계가 아니다\n",
    "- '단어 기준'일 수도, '문자 기준'일 수도, 혹은 그 중간인 **서브워드(subword)** 기준일 수도 있다.\n",
    "- BPE(Byte Pair Encoding), WordPiece, SentencePiece 등은 인간 언어의 **패턴성과 압축 가능성**을 이용한 전략들이다.\n",
    "- 핵심은: **의미의 단위와 연산 효율성 사이의 절충**을 찾는 일이다.\n",
    "\n",
    "> \"Tokenizer는 인간의 문장을 숫자의 나열로 변환하는 과정이 아니라, 의미의 가장 작은 조각들을 기계가 다룰 수 있도록 재해석하는 첫 번째 필터이다.\"\n",
    "\n",
    "### 🔸 수학적 시각 (정보이론 관점)\n",
    "\n",
    "좋은 토크나이저는 모델이 학습하기 쉽게 만드는 분할을 수행한다. 이때, 자주 등장하는 패턴을 재사용하는 방식은 **엔트로피 감소**와 연결된다.\n",
    "\n",
    "$$\n",
    "H(T) = -\\sum_{i} p(t_i) \\log p(t_i)\n",
    "$$\n",
    "\n",
    "- 여기서 $T$는 토큰 시퀀스이며, $p(t_i)$는 토큰 $t_i$의 등장 확률이다.\n",
    "- 자주 등장하는 서브워드를 병합하면 엔트로피를 줄일 수 있어 모델 학습이 안정된다.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Embedding: 토큰에 의미를 부여하는 수치적 재구성\n",
    "\n",
    "- 우리가 말하는 ‘단어’ 하나하나는 모델 입장에서는 그대로 이해할 수 없다.\n",
    "그래서 단어를 숫자의 집합으로 바꾸어야 하고, 이 과정을 벡터로 변환한다고 표현한다.\n",
    "- 예를 들어 “apple”이라는 단어는 [0.12, -1.03, 0.57, ..., 0.88] 같은 숫자 리스트로 바뀌어 저장된다.\n",
    "\n",
    "> 토크나이저가 언어를 잘게 나누는 역할이라면, 임베딩은 그렇게 나눠진 조각들 사이에 의미적 유사성과 차이를 표현할 수 있도록 구조를 부여하는 과정이다.\n",
    "\n",
    "\n",
    "### 🔹 정의: 의미 공간 위로의 매핑\n",
    "\n",
    "각 토큰 $t_i$는 다음과 같은 함수 $f$에 의해 벡터 공간의 점으로 변환된다.\n",
    "\n",
    "$$\n",
    "f: t_i \\mapsto \\mathbf{v}_i \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "즉, 토큰은 $d$차원의 벡터로 표현되며, 이 벡터들 간의 거리나 방향은 의미적인 유사성을 내포한다.\n",
    "\n",
    "### 🔹 유사성 계산\n",
    "\n",
    "자주 사용되는 의미 유사성 측도는 **코사인 유사도**이다.\n",
    "\n",
    "$$\n",
    "\\text{sim}(\\mathbf{v}_i, \\mathbf{v}_j) = \\frac{\\mathbf{v}_i \\cdot \\mathbf{v}_j}{\\|\\mathbf{v}_i\\| \\|\\mathbf{v}_j\\|}\n",
    "$$\n",
    "\n",
    "- 1에 가까울수록 유사한 의미를 가진다.\n",
    "- 이는 임베딩 공간이 단순한 임의의 숫자 공간이 아니라 **기하학적 관계**가 보존되는 구조적 공간임을 의미한다.\n",
    "\n",
    "> \"Embedding은 언어를 공간 위로 펼쳐내는 작업이다. 단어들은 더 이상 기호가 아니라, 구조 속에서 관계를 맺는 점(point)이 된다.\"\n",
    "\n",
    "### 🔸 학습 가능한 파라미터로서의 임베딩\n",
    "\n",
    "임베딩은 고정된 것이 아니라, 학습된다. 보통은 임베딩 행렬 $E \\in \\mathbb{R}^{V \\times d}$를 사용한다.\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_i = E[i]\n",
    "$$\n",
    "\n",
    "- $V$: vocabulary size\n",
    "- $d$: embedding 차원 수\n",
    "- 각 token ID $i$는 이 행렬의 한 row를 참조한다.\n",
    "\n",
    "하지만 이 값은 역전파(backpropagation)에 의해 계속해서 **업데이트**되며, 모델의 표현력을 구성하는 핵심 요소가 된다.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 토크나이저와 임베딩의 결합: 기호에서 의미로의 이동\n",
    "\n",
    "- Tokenizer는 **무한한 언어의 흐름**을 **유한한 조각들**로 쪼개고,\n",
    "- Embedding은 이 조각들에 **맥락 속 좌표**를 부여한다.\n",
    "\n",
    "이 두 과정은 연결되어 있으나, 본질적으로 역할이 다르다.\n",
    "\n",
    "| 역할 | Tokenizer | Embedding |\n",
    "|------|-----------|-----------|\n",
    "| 목적 | 문장을 기계가 다룰 수 있는 조각으로 분리 | 조각들에 의미적 구조를 부여 |\n",
    "| 관점 | 구조화의 시작 | 구조화의 맥락화 |\n",
    "| 수학 | 확률 기반 분할 (엔트로피) | 벡터 공간 변환 ($\\mathbb{R}^d$) |\n",
    "| 결과 | token id 시퀀스 | dense vector 시퀀스 |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 다음 주제로 이어지는 관문\n",
    "\n",
    "- **Positional Encoding**: 임베딩된 벡터에 ‘순서’라는 요소를 다시 불어넣는 작업\n",
    "- **Attention**: 벡터들 간의 관계를 동적으로 계산하며, 어떤 벡터가 다른 벡터에 얼마나 집중해야 하는지를 결정\n",
    "- **Transformer**: 토크나이저-임베딩-포지셔널 인코딩-어텐션이 하나의 유기적 구조로 작동하는 프레임워크\n",
    "\n",
    "---\n",
    "\n",
    "> \"Tokenizer는 언어의 물리적 조각을 만들고, Embedding은 그 조각들에 의미의 방향을 부여한다. 이것은 기호에서 구조로, 구조에서 의미로 나아가는 여정이다.\"\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6176c1-362a-48c6-9206-5bcf407a256a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
